{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from source.utils.config_manager import ConfigManager\n",
    "\n",
    "#hyperparameters tuning\n",
    "from ray import tune\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "from source.hyperparams_tuning import HyperparameteresTunner\n",
    "\n",
    "# transformers\n",
    "from source.transformers import TestTransformersFactory\n",
    "from source.transformers import TrainTrasformersFactory\n",
    "\n",
    "# data loaders\n",
    "from source.dataloaders.cnn_data_loaders_factory import CNNDataLoaderFactory\n",
    "\n",
    "\n",
    "# networks\n",
    "from source.custom_cnn.pytorch_ray_tutorial_cnn import Net\n",
    "from source.custom_cnn.dropout_batch_norm_cnn import DBN_cnn\n",
    "from source.pre_trained.vgg16 import PretrainedVGG16_cnn\n",
    "from source.pre_trained.resnet_cnn import PretrainedRESNET_cnn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpus_per_trial = 2\n",
    "max_num_epochs = 40\n",
    "num_samples = 10\n",
    "dataset_name = 'cifar10'\n",
    "dataset_path = ConfigManager.get_dataset_path(dataset_name)\n",
    "local_dir = ConfigManager.get_tuning_results_path()\n",
    "    \n",
    "config_criterion_1 = {\n",
    "    \"type\": tune.choice([nn.CrossEntropyLoss])\n",
    "}\n",
    "\n",
    "config_optimizer_1 = {\n",
    "    \"type\": tune.choice([optim.Adam]),\n",
    "    \"lr\":0.001,\n",
    "    \"weight_decay\": 0.0005\n",
    "    \n",
    "}\n",
    "\n",
    "config_net_1 = {\n",
    "    \"type\": tune.choice([PretrainedVGG16_cnn]),\n",
    "    \"optimizer\": tune.choice([config_optimizer_1]),\n",
    "    \"criterion\": tune.choice([config_criterion_1])}\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"data_loaders_factory\": tune.choice([\n",
    "        CNNDataLoaderFactory(dataset_path,\n",
    "            TrainTrasformersFactory.get_transformer_kaggle(), TestTransformersFactory.get_transformer_kaggle())\n",
    "    ]),\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 40,\n",
    "    \"net\": tune.choice([config_net_1]),\n",
    "    \"tuning_id\": \"XYZ\"\n",
    "}\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=max_num_epochs,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2)\n",
    "\n",
    "reporter = JupyterNotebookReporter(max_progress_rows=20, overwrite=True, max_report_frequency=10, max_error_rows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameteresTunner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-03 17:16:04 (running for 00:04:26.42)<br>Memory usage on this node: 13.8/15.7 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/0.77 GiB heap, 0.0/0.39 GiB objects<br>Result logdir: c:\\content\\drive\\MyDrive\\Github\\cifar10-computer_vision\\tuning_results\\__train_validate_2022-04-03_17-11-37<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc  </th><th>data_loaders_factory  </th><th>net                                                                                                                                                                                                                                   </th><th>net/criterion                                             </th><th>net/criterion/type                              </th><th>net/optimizer                                                                 </th><th>net/optimizer/type             </th><th>net/type                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>__train_validate_5b8f2_00000</td><td>PENDING </td><td>     </td><td>Train_transformer:Compose(\n",
       "    Resize(size=(244, 244), interpolation=bilinear, max_size=None, antialias=None)\n",
       "    RandomHorizontalFlip(p=0.5)\n",
       "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
       "    RandomAffine(degrees=[0.0, 0.0], scale=(0.8, 1.2), shear=[-10.0, 10.0])\n",
       "    ColorJitter(brightness=[0.8, 1.2], contrast=[0.8, 1.2], saturation=[0.8, 1.2], hue=None)\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "); Test_transformer:Compose(\n",
       "    Resize(size=(244, 244), interpolation=bilinear, max_size=None, antialias=None)\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       ")                       </td><td>{&#x27;type&#x27;: &lt;class &#x27;source.pre_trained.vgg16.PretrainedVGG16_cnn&#x27;&gt;, &#x27;optimizer&#x27;: {&#x27;type&#x27;: &lt;class &#x27;torch.optim.adam.Adam&#x27;&gt;, &#x27;lr&#x27;: 0.001, &#x27;weight_decay&#x27;: 0.0005}, &#x27;criterion&#x27;: {&#x27;type&#x27;: &lt;class &#x27;torch.nn.modules.loss.CrossEntropyLoss&#x27;&gt;}}</td><td>{&#x27;type&#x27;: &lt;class &#x27;torch.nn.modules.loss.CrossEntropyLoss&#x27;&gt;}</td><td>&lt;class &#x27;torch.nn.modules.loss.CrossEntropyLoss&#x27;&gt;</td><td>{&#x27;type&#x27;: &lt;class &#x27;torch.optim.adam.Adam&#x27;&gt;, &#x27;lr&#x27;: 0.001, &#x27;weight_decay&#x27;: 0.0005}</td><td>&lt;class &#x27;torch.optim.adam.Adam&#x27;&gt;</td><td>&lt;class &#x27;source.pre_trained.vgg16.PretrainedVGG16_cnn&#x27;&gt;</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 17:16:04,293\tERROR tune.py:635 -- Trials did not complete: [__train_validate_5b8f2_00000]\n",
      "2022-04-03 17:16:04,293\tINFO tune.py:639 -- Total run time: 266.70 seconds (266.41 seconds for the tuning loop).\n",
      "2022-04-03 17:16:04,294\tWARNING tune.py:643 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n",
      "2022-04-03 17:16:04,299\tWARNING experiment_analysis.py:533 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15676/611397153.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreporter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\gosia\\Desktop\\studia\\mgr_sem_1\\DL\\cifar10-computer_vision\\source\\hyperparams_tuning.py\u001b[0m in \u001b[0;36mtune\u001b[1;34m(self, dataset, local_dir, config, scheduler, reporter, num_samples, resources_per_trial, device)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mbest_trial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_best_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"min\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"last\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best trial config: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         print(\"Best trial final validation loss: {}\".format(\n\u001b[0;32m     29\u001b[0m             best_trial.last_result[\"loss\"]))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "best_model = tuner.tune(dataset_name, local_dir, config, scheduler, reporter, num_samples=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f596b1f1e2414a57cbac219f0e45f8be2c707cf8cf81cba26a1cb9bf52f74de"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
